{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.retrievers import MergerRetriever, ContextualCompressionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "print(\"Embeddings loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presets using Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path resume/VishnuPrakash_Resume.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading Resume\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m resume \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume/VishnuPrakash_Resume.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m resume_load \u001b[38;5;241m=\u001b[39m resume\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      5\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:157\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:100\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path resume/VishnuPrakash_Resume.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# Loading Resume\n",
    "resume = PyPDFLoader(\"resume/VishnuPrakash_Resume.pdf\")\n",
    "resume_load = resume.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "resume_splitted = text_splitter.split_documents(resume_load)\n",
    "\n",
    "print(resume_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "# loading job description\n",
    "path = \"Testing/resume/job_desc.txt\"\n",
    "with open(path, \"r\") as file:\n",
    "    job_desc = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "jd_splitted = text_splitter.split_text(job_desc)\n",
    "\n",
    "print(jd_splitted[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# vectorDB for resume\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chroma_resume \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      3\u001b[0m         resume_splitted,embeddings,\n\u001b[1;32m      4\u001b[0m         collection_metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m},persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_storage/resume_store\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# l2 is the default\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# vectorDB for JD\u001b[39;00m\n\u001b[1;32m      7\u001b[0m chroma_jd \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m      8\u001b[0m     jd_splitted,embeddings,collection_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      9\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_storage/jd_store\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chroma' is not defined"
     ]
    }
   ],
   "source": [
    "# vectorDB for resume\n",
    "chroma_resume = Chroma.from_documents(\n",
    "        resume_splitted,embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},persist_directory=\"vector_storage/resume_store\" # l2 is the default\n",
    "    )\n",
    "# vectorDB for JD\n",
    "chroma_jd = Chroma.from_texts(\n",
    "    jd_splitted,embeddings,collection_metadata = {\"hnsw:space\": \"cosine\"},\n",
    "    persist_directory=\"vector_storage/jd_store\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vector stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chroma_resume = Chroma(persist_directory=\"vector_storage/resume_store\",embedding_function=embeddings)\n",
    "load_chroma_jd = Chroma(persist_directory=\"vector_storage/jd_store\",embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Merge Retriever and Perform semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_resume = load_chroma_resume.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})\n",
    "\n",
    "retriever_jd = load_chroma_jd.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = MergerRetriever(retrievers=[retriever_resume, retriever_jd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISHNU PRAKASH\n",
      "Data Scientist\n",
      "vishnucheppanam@gmail.com — +91 80 780 43398\n",
      "linkedin.com/in/vishnuprksh — github.com/vishnuprksh — vishnuprakash.online\n",
      "PROFILE\n",
      "Data Science aspirant with hands-on experience in machine learning and deep learning, seeking a\n",
      "challenging role as a Data Scientist/Machine Learning Engineer. Eager to leverage skills in Artificial\n",
      "Intelligence, Neural Networks, and Algorithms to contribute to the companies and personal growth.\n",
      "PROJECTS\n",
      "GemInsights Git Link , Automating Exploratory Data Analysis (EDA) and Insight generation with\n",
      "the help of latest Gemini engine by Google\n",
      "•EDA generation with the help of AutoViz and Image analysis with Gemini-Pro-Vision.\n",
      "•Hallucination check with Trulense-Eval.\n",
      "•UI with Streamlit.\n",
      "TradePilot Git Link , Empowering stock market price prediction with sentiment analysis from News,\n",
      "Twitter and Reddit\n",
      "•Stock price history collected from Yahoo Finance and Text data scraped from Economic Times,\n",
      "Reddit, and Twitter.\n",
      "•Sentiment analysis with Finbert (Hugging Face) and Summary with Google Gemini-Pro.\n",
      "•UI with FastAPI.\n",
      "SKILLS\n",
      "Machine Learning ( scikit ,NLTK ,spaCy ,Numpy ,Pandas ,NLP )\n",
      "Deep Learning ( Keras ,PyTorch ,TensorFlow ,RNN ,LSTM ,CNN )\n",
      "Data Visualization ( Tableau ,PowerBI )\n",
      "Database Management ( MySQL ,MongoDB )\n",
      "Programming Languages ( Python ,JavaScript ,C,C++)\n",
      "UI Development ( FastAPI ,HTML ,CSS,Streamlit ,Django ).\n",
      "EDUCATION\n",
      "Data Science Self Learning Bootcamp, Brototype Ernakulam, India\n",
      "•Covered topics including Data Science, Machine Learning, Deep Learning, Generative AI,\n",
      "NLP, Feature Engineering, RAG Modeling, Data Structures, Statistics and Probability, Web\n",
      "Development.\n",
      "•Trained in communication and personality development.\n",
      "B.Tech in Naval Architecture, CUSAT Kerala, India\n",
      "•Studied Ship Building, Stability, Hydrodynamics, Mechanics, Graphics, and Computer Science.\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Naval Architect, SEDS 07/2020 – 03/2023 — Kochi, India\n",
      "•Contributed to the development of shipbuilding projects, adhering to industry standards.\n",
      "Production Supervisor, Navgathi 07/2019 – 03/2020 — Kochi, India\n",
      "•Managed the production team for boat production.\n",
      "CERTIFICATES\n",
      "Machine Learning (Kaggle) ,Data Science (Coursera) ,HTML (Codeacademy) .\n",
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "for chunks in merged.get_relevant_documents(\"Machine Learning\"):\n",
    "    print(chunks.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
